{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, \\\n",
    "    average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import os\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import copy\n",
    "from torch import optim, cuda\n",
    "import pandas as pd\n",
    "import glob\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "# Useful for examining network\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "# from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import __add__\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from ignite.engine import Engine\n",
    "from ignite.engine import create_supervised_evaluator\n",
    "from ignite.engine import create_supervised_trainer\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall, Fbeta\n",
    "from ignite.contrib.metrics.roc_auc import ROC_AUC\n",
    "from ignite.contrib.metrics.regression import R2Score\n",
    "from ignite.handlers import ModelCheckpoint, global_step_from_engine, Checkpoint, DiskSaver\n",
    "from ignite.handlers.early_stopping import EarlyStopping\n",
    "from ignite.contrib.handlers import TensorboardLogger\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "val_metrics = {\n",
    "        \"accuracy\": Accuracy(output_transform=thresholded_output_transform),\n",
    "        \"loss\": Loss(self.criterion),\n",
    "        \"roc_auc\": ROC_AUC(output_transform=thresholded_output_transform),\n",
    "        \"precision\": Precision(output_transform=thresholded_output_transform),\n",
    "        \"precision_0\": Precision(output_transform=class0_thresholded_output_transform),\n",
    "        \"recall\": Recall(output_transform=thresholded_output_transform),\n",
    "        \"recall_0\": Recall(output_transform=class0_thresholded_output_transform),\n",
    "        }\n",
    "    val_metrics[\"f1\"]=Fbeta(beta=1.0, average=False, precision=val_metrics['precision'], recall=val_metrics['recall'])\n",
    "    val_metrics[\"f1_0\"]=Fbeta(beta=1.0, average=False, precision=val_metrics['precision_0'], recall=val_metrics['recall_0'])\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "def get_data_loaders(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "    batch_size = 20\n",
    "    dlen = X_train.shape[0]\n",
    "\n",
    "\n",
    "    y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "    X_test = TensorDataset(torch.FloatTensor(X_test), y_test)\n",
    "    test_loader = DataLoader(X_test, batch_size=batch_size, pin_memory=True, shuffle=True,num_workers = 10)\n",
    "\n",
    "    y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_train = TensorDataset(torch.FloatTensor(X_train), y_train)\n",
    "    train_loader = DataLoader(X_train, batch_size=batch_size, pin_memory=True, shuffle=True,num_workers = 10)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def get_criterion(y_train):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Train on: {device}')\n",
    "\n",
    "\n",
    "    LABEL_WEIGHTS = []\n",
    "\n",
    "   \n",
    "    class_counts = np.bincount(y_train).tolist() #y_train.value_counts().tolist()\n",
    "    weights = torch.tensor(np.array(class_counts) / sum(class_counts))\n",
    "    # assert weights[0] > weights[1]\n",
    "    print(\"CLASS 0: {}, CLASS 1: {}\".format(weights[0], weights[1]))\n",
    "    weights = weights[0] / weights\n",
    "    print(\"WEIGHT 0: {}, WEIGHT 1: {}\".format(weights[0], weights[1]))\n",
    "    LABEL_WEIGHTS.append(weights[1])\n",
    "\n",
    "    print(\"Label Weights: \", LABEL_WEIGHTS)\n",
    "    cuda_idx = 0\n",
    "    LABEL_WEIGHTS = torch.stack(LABEL_WEIGHTS)\n",
    "    LABEL_WEIGHTS = LABEL_WEIGHTS.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=LABEL_WEIGHTS)\n",
    "    criterion.to(device)\n",
    "    \n",
    "    return criterion\n",
    "\n",
    "def thresholded_output_transform(output):\n",
    "            y_pred, y = output\n",
    "            y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "            return y_pred, y\n",
    "def class0_thresholded_output_transform(output):\n",
    "            y_pred, y = output\n",
    "            y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "            y=1-y\n",
    "            y_pred=1-y_pred\n",
    "            return y_pred, y\n",
    "def class0_thresholded_output_transform_mayo(output):\n",
    "            y_pred, y = output\n",
    "            _, y_pred = torch.max(y_pred.data, 1)\n",
    "            y=1-y\n",
    "            y_pred=1-y_pred\n",
    "            return y_pred, y\n",
    "def thresholded_output_transform_mayo(output):\n",
    "            y_pred, y = output\n",
    "            _, y_pred = torch.max(y_pred.data, 1)\n",
    "            return y_pred, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, model_name, criterion, train_loader, test_loader, optimizers, lr_lower, \n",
    "                 lr_upper, metric, max_epochs, early_stopping_patience=None, lr_scheduler=False, \n",
    "                 step_size=None, gamma=None,Project_name = None,model_origin = None,reduce = False,\n",
    "                 depth = [1,2,2,3,3,3,3],channels = [32,16,24,40,80,112,192,320,1280,1867],\n",
    "                 dilation = 1,stride = 2,expansion = 6,additional_inputs = 0,category = 'roc',\n",
    "                 multi=False,reg = False, start_channels=12, multi_class=False):\n",
    "        # Hold this implementation specific arguments as the fields of the class.\n",
    "        self.model_name=model_name\n",
    "        self.train_loader=train_loader\n",
    "        self.test_loader=test_loader\n",
    "        self.optimizers = optimizers\n",
    "        self.criterion=criterion\n",
    "        self.metric = metric\n",
    "        self.max_epochs=max_epochs\n",
    "        self.lr_lower=lr_lower\n",
    "        self.lr_upper=lr_upper\n",
    "        self.early_stopping_patience=early_stopping_patience\n",
    "        self.lr_scheduler=lr_scheduler\n",
    "        self.step_size=step_size\n",
    "        self.gamma=gamma\n",
    "        self.Project_name = Project_name\n",
    "        self.model_origin = model_origin\n",
    "        self.reduce = reduce\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.start_channels = start_channels\n",
    "        self.dilation = dilation\n",
    "        self.stride = stride\n",
    "        self.exp = expansion\n",
    "        self.additional_inputs = additional_inputs\n",
    "        self.type = category\n",
    "        if self.type == 'regression':\n",
    "            self.metric = 'loss'\n",
    "        print('self.exp',self.exp)\n",
    "        self.multi_class=multi_class\n",
    "        self.multi=multi\n",
    "        self.reg = reg\n",
    "    def __call__(self, trial):\n",
    "        torch.cuda.empty_cache()\n",
    "        # load model\n",
    "        model = None\n",
    "        if self.model_origin == 'Columbia':\n",
    "            model = getattr(models, self.model_name)(inital_kernel_num = 32,dropout = 0.6,conv1kernel1 = 7,conv1kernel2 = 3,reduced = self.reduce)\n",
    "        elif self.model_origin == 'Lancet':\n",
    "            model_type='lancet'\n",
    "            is_2d=False\n",
    "            class_name = 'hemoglobin'\n",
    "            lr = 1e-3\n",
    "            model_name = 'lancet'\n",
    "            binary=False\n",
    "            batch_norm=True\n",
    "            drop_prob=0.\n",
    "            conv_width=3\n",
    "            train_delta=0\n",
    "            first_layer_out_channels=None\n",
    "            first_layer_kernel_size=None\n",
    "            normalize_x=\"lead\"\n",
    "            binary_cutoff = None\n",
    "            normalize_y=False\n",
    "            from ecgnet import model \n",
    "            reload(model)\n",
    "            model = model.build_model(model_type, 1, 12 if is_2d else 12, is_2d=is_2d, binary=binary, batch_norm=batch_norm, default_conv_kernel=conv_width, drop_prob=drop_prob,\n",
    "                                     binary_cutoff=binary_cutoff, first_layer_out_channels=first_layer_out_channels, first_layer_kernel_size=first_layer_kernel_size)\n",
    "        elif self.model_origin == 'mayo':\n",
    "            model = models.Mayo_Net()\n",
    "        elif self.model_origin == 'eff2d':\n",
    "            \n",
    "            model=model.EfficientNet.from_name('efficientnet-b0')\n",
    "            model._fc = nn.Linear(1280,1)\n",
    "        elif self.model_origin == 'RCRI_Net':\n",
    "            model = models.EffNet(depth = self.depth,channels = self.channels,dilation = self.dilation,\n",
    "                                  stride = self.stride,expansion = self.exp, num_additional_features=self.additional_inputs,\n",
    "                                 multi_class = self.multi_class, reg = self.type=='regression', start_channels=self.start_channels)\n",
    "            print(model)\n",
    "            print('model: ' +str(self.depth)+\" channels: \"+str(self.channels)+str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "        if self.model_origin == 'Old Columbia':\n",
    "            import old_models\n",
    "            model = getattr(old_models, self.model_name)(inital_kernel_num = 32,dropout = 0.6,conv1kernel1 = 7,conv1kernel2 = 3)\n",
    "        if self.model_origin == 'Wacky':\n",
    "            import old_models\n",
    "            model = getattr(old_models, 'Wacky')(inital_kernel_num = 32,dropout = 0.6,conv1kernel1 = 7,conv1kernel2 = 3)\n",
    "            print(model)\n",
    "            print('model: ' +str(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "            \n",
    "        wandb.watch(model)\n",
    "\n",
    "        # print(model)\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "            model.to(device)\n",
    "            model = nn.DataParallel(model)\n",
    "        if self.type == 'regression':\n",
    "            val_metrics = {\n",
    "            \"loss\": Loss(self.criterion),\n",
    "            \"r2\": R2Score()}\n",
    "        else:\n",
    "            val_metrics = {\n",
    "                \"loss\": Loss(self.criterion),\n",
    "                \"roc_auc\": ROC_AUC()\n",
    "                }\n",
    "            \n",
    "\n",
    "\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", self.optimizers)\n",
    "        learnrate = trial.suggest_loguniform(\"lr\", self.lr_lower, self.lr_upper)\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learnrate)\n",
    "        trainer = create_supervised_trainer(model, optimizer, self.criterion, device=device)\n",
    "        ProgressBar().attach(trainer)\n",
    "        train_evaluator = create_supervised_evaluator(model, metrics= val_metrics, device=torch.device('cuda'))\n",
    "        evaluator = create_supervised_evaluator(model, metrics= val_metrics, device=torch.device('cuda'))\n",
    "\n",
    "        # Register a pruning handler to the evaluator.\n",
    "        pruning_handler = optuna.integration.PyTorchIgnitePruningHandler(trial, self.metric, trainer)\n",
    "        evaluator.add_event_handler(Events.COMPLETED, pruning_handler)\n",
    "        if not self.type == 'regression':\n",
    "            def score_fn(engine):\n",
    "                if self.type == 'regression':\n",
    "                    score = engine.state.metrics['loss']\n",
    "                    return score\n",
    "                else:\n",
    "                    score = engine.state.metrics[self.metric]\n",
    "                return score if self.metric!='loss' else -score\n",
    "        def score_fn_2(engine):\n",
    "            score = engine.state.metrics['loss']\n",
    "            return -score\n",
    "\n",
    "        #early stopping\n",
    "        if not self.type == 'regression':\n",
    "            if self.early_stopping_patience is not None:\n",
    "                es_handler = EarlyStopping(patience=self.early_stopping_patience, score_function=score_fn, trainer=trainer)\n",
    "                evaluator.add_event_handler(Events.COMPLETED, es_handler)\n",
    "        else:\n",
    "            if self.early_stopping_patience is not None:\n",
    "                es_handler = EarlyStopping(patience=self.early_stopping_patience, score_function=score_fn_2, trainer=trainer)\n",
    "                evaluator.add_event_handler(Events.COMPLETED, es_handler)\n",
    "\n",
    "        #checkpointing\n",
    "        to_save = {'model': model}\n",
    "\n",
    "        checkpointname=self.Project_name+'/checkpoint'\n",
    "        for key, value in trial.params.items():\n",
    "          checkpointname+=key+': '+str(value)+', '\n",
    "        if not self.type == 'regression':\n",
    "            checkpoint_handler = Checkpoint(to_save, DiskSaver(checkpointname, create_dir=True,require_empty=False),\n",
    "                             filename_prefix='best_roc', score_function=score_fn, score_name=\"val_roc\",\n",
    "                             global_step_transform=global_step_from_engine(trainer))\n",
    "        else:\n",
    "            checkpoint_handler = Checkpoint(to_save, DiskSaver(checkpointname, create_dir=True,require_empty=False),\n",
    "                             filename_prefix='best_roc', score_function=score_fn_2, score_name=\"val_roc\",\n",
    "                             global_step_transform=global_step_from_engine(trainer))\n",
    "        checkpoint_handler_2 = Checkpoint(to_save, DiskSaver(checkpointname, create_dir=True,require_empty=False),\n",
    "                         filename_prefix='best_loss', score_function=score_fn_2, score_name=\"val_loss\",\n",
    "                         global_step_transform=global_step_from_engine(trainer))\n",
    "        wandb.log({'Checkpoint Name':checkpointname},commit=False)\n",
    "        print(checkpointname)\n",
    "        evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n",
    "        evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler_2)\n",
    "\n",
    "        #  Add lr scheduler\n",
    "        if self.lr_scheduler is True:\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=self.step_size, gamma=self.gamma)\n",
    "            trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda engine: scheduler.step())\n",
    "            print(scheduler)\n",
    "\n",
    "        \n",
    "        \n",
    "        #print metrics on each epoch completed\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_training_results(engine):\n",
    "            train_evaluator.run(self.train_loader)\n",
    "            metrics = train_evaluator.state.metrics\n",
    "            if self.type == 'regression':\n",
    "                wandb.log({'Train loss':metrics[\"loss\"],\n",
    "                          'Train r2':metrics['r2']},commit=False)\n",
    "            else:\n",
    "                wandb.log({'Train loss':metrics[\"loss\"],'Train roc_auc':metrics['roc_auc']},commit=False)\n",
    "          \n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_validation_results(engine):\n",
    "            evaluator.run(self.test_loader)\n",
    "            metrics = evaluator.state.metrics\n",
    "            if self.type == 'regression':\n",
    "                wandb.log({'Val loss':metrics[\"loss\"],\n",
    "                          'Val r2':metrics[\"r2\"]})\n",
    "            else:\n",
    "                wandb.log({'Val loss':metrics[\"loss\"],'Val roc_auc':metrics['roc_auc']})\n",
    "\n",
    "        #Tensorboard logs\n",
    "\n",
    "\n",
    "        #run the trainer\n",
    "        trainer.run(self.train_loader, max_epochs=self.max_epochs)\n",
    "\n",
    "        #load the checkpoint with the best validation metric in the trial\n",
    "        to_load = to_save\n",
    "        checkpoint = torch.load(checkpointname+'/'+checkpoint_handler.last_checkpoint)\n",
    "        Checkpoint.load_objects(to_load=to_load, checkpoint=checkpoint)\n",
    "\n",
    "        evaluator.run(self.test_loader)\n",
    "        if self.type == 'regression':\n",
    "            return evaluator.state.metrics['loss']\n",
    "        return evaluator.state.metrics[self.metric]\n",
    "\n",
    "\n",
    "def run_trials(objective, pruner, num_trials, direction,project_name): \n",
    "    wandb.init(project=project_name,settings=wandb.Settings(start_method='thread'))\n",
    "    \n",
    "    pruner = pruner\n",
    "    study = optuna.create_study(direction=direction, pruner=pruner)\n",
    "    study.optimize(objective, n_trials=num_trials, gc_after_trial=True,show_progress_bar=False)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "          print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
